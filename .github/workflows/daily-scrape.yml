name: Daily Scraper

on:
  schedule:
    # Run at 2 AM UTC every day
    - cron: '0 2 * * *'
  workflow_dispatch: # Allow manual trigger
    inputs:
      max_pages:
        description: 'Maximum pages to scrape per source'
        required: false
        default: '2'
        type: string

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          
      - name: Install dependencies
        run: npm ci
        
      - name: Run scraper
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          BRIGHTDATA_USERNAME: ${{ secrets.BRIGHTDATA_USERNAME }}
          BRIGHTDATA_PASSWORD: ${{ secrets.BRIGHTDATA_PASSWORD }}
          BRIGHTDATA_SCRAPING_BROWSER_PASSWORD: ${{ secrets.BRIGHTDATA_SCRAPING_BROWSER_PASSWORD }}
        run: |
          MAX_PAGES="${{ github.event.inputs.max_pages || '2' }}"
          echo "Running scraper with max-pages=$MAX_PAGES"
          npx tsx scripts/scrape-and-save.ts --max-pages=$MAX_PAGES
          
      - name: Notify on failure
        if: failure()
        run: |
          echo "Scraper failed at $(date)" >> scraper-failures.log