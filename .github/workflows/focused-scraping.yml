name: Focused Model Scraping (Rotating Sources)

on:
  # Run multiple times per day for different models, rotating sources by day of week
  schedule:
    # Mon/Thu: BaT + Classic.com
    - cron: '0 1 * * 1,4'   # 1 AM: 911 GT models
    - cron: '0 3 * * 1,4'   # 3 AM: 911 996 generation
    - cron: '0 5 * * 1,4'   # 5 AM: 911 997 generation
    - cron: '0 7 * * 1,4'   # 7 AM: 911 991 generation
    - cron: '0 9 * * 1,4'   # 9 AM: 911 992 generation
    - cron: '0 11 * * 1,4'  # 11 AM: Cayman models
    - cron: '0 13 * * 1,4'  # 1 PM: Boxster models

    # Tue/Fri: Cars & Bids + Classic.com
    - cron: '0 1 * * 2,5'   # 1 AM: 911 GT models
    - cron: '0 3 * * 2,5'   # 3 AM: 911 996 generation
    - cron: '0 5 * * 2,5'   # 5 AM: 911 997 generation
    - cron: '0 7 * * 2,5'   # 7 AM: 911 991 generation
    - cron: '0 9 * * 2,5'   # 9 AM: 911 992 generation
    - cron: '0 11 * * 2,5'  # 11 AM: Cayman models
    - cron: '0 13 * * 2,5'  # 1 PM: Boxster models

    # Wed/Sat: BaT + Cars & Bids
    - cron: '0 1 * * 3,6'   # 1 AM: 911 GT models
    - cron: '0 3 * * 3,6'   # 3 AM: 911 996 generation
    - cron: '0 5 * * 3,6'   # 5 AM: 911 997 generation
    - cron: '0 7 * * 3,6'   # 7 AM: 911 991 generation
    - cron: '0 9 * * 3,6'   # 9 AM: 911 992 generation
    - cron: '0 11 * * 3,6'  # 11 AM: Cayman models
    - cron: '0 13 * * 3,6'  # 1 PM: Boxster models

    # Sun: All sources (comprehensive sweep)
    - cron: '0 1 * * 0'     # 1 AM: 911 GT models
    - cron: '0 3 * * 0'     # 3 AM: 911 996 generation
    - cron: '0 5 * * 0'     # 5 AM: 911 997 generation
    - cron: '0 7 * * 0'     # 7 AM: 911 991 generation
    - cron: '0 9 * * 0'     # 9 AM: 911 992 generation
    - cron: '0 11 * * 0'    # 11 AM: Cayman models
    - cron: '0 13 * * 0'    # 1 PM: Boxster models

  # Allow manual triggering
  workflow_dispatch:
    inputs:
      model:
        description: 'Model to scrape'
        required: true
        type: choice
        options:
          - 911-gt
          - 911-996
          - 911-997
          - 911-991
          - 911-992
          - cayman
          - boxster
      max_pages:
        description: 'Maximum pages to scrape'
        required: false
        default: '1'
        type: string

env:
  NODE_ENV: production

jobs:
  determine-config:
    name: Determine Scraping Config
    runs-on: ubuntu-latest
    outputs:
      model: ${{ steps.set-config.outputs.model }}
      scrape_bat: ${{ steps.set-config.outputs.scrape_bat }}
      scrape_carsandbids: ${{ steps.set-config.outputs.scrape_carsandbids }}
      scrape_classic: ${{ steps.set-config.outputs.scrape_classic }}
    steps:
      - name: Set config based on schedule
        id: set-config
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            # Manual trigger
            echo "model=${{ github.event.inputs.model }}" >> $GITHUB_OUTPUT
            echo "scrape_bat=true" >> $GITHUB_OUTPUT
            echo "scrape_carsandbids=true" >> $GITHUB_OUTPUT
            echo "scrape_classic=true" >> $GITHUB_OUTPUT
          else
            # Determine model based on hour
            HOUR=$(date -u +%H)
            if [ "$HOUR" == "01" ]; then
              echo "model=911-gt" >> $GITHUB_OUTPUT
            elif [ "$HOUR" == "03" ]; then
              echo "model=911-996" >> $GITHUB_OUTPUT
            elif [ "$HOUR" == "05" ]; then
              echo "model=911-997" >> $GITHUB_OUTPUT
            elif [ "$HOUR" == "07" ]; then
              echo "model=911-991" >> $GITHUB_OUTPUT
            elif [ "$HOUR" == "09" ]; then
              echo "model=911-992" >> $GITHUB_OUTPUT
            elif [ "$HOUR" == "11" ]; then
              echo "model=cayman" >> $GITHUB_OUTPUT
            elif [ "$HOUR" == "13" ]; then
              echo "model=boxster" >> $GITHUB_OUTPUT
            else
              echo "model=911-gt" >> $GITHUB_OUTPUT
            fi

            # Determine sources based on day of week
            DOW=$(date -u +%u)  # 1=Monday, 7=Sunday
            if [ "$DOW" == "1" ] || [ "$DOW" == "4" ]; then
              # Mon/Thu: BaT + Classic.com
              echo "scrape_bat=true" >> $GITHUB_OUTPUT
              echo "scrape_carsandbids=false" >> $GITHUB_OUTPUT
              echo "scrape_classic=true" >> $GITHUB_OUTPUT
            elif [ "$DOW" == "2" ] || [ "$DOW" == "5" ]; then
              # Tue/Fri: Cars & Bids + Classic.com
              echo "scrape_bat=false" >> $GITHUB_OUTPUT
              echo "scrape_carsandbids=true" >> $GITHUB_OUTPUT
              echo "scrape_classic=true" >> $GITHUB_OUTPUT
            elif [ "$DOW" == "3" ] || [ "$DOW" == "6" ]; then
              # Wed/Sat: BaT + Cars & Bids
              echo "scrape_bat=true" >> $GITHUB_OUTPUT
              echo "scrape_carsandbids=true" >> $GITHUB_OUTPUT
              echo "scrape_classic=false" >> $GITHUB_OUTPUT
            else
              # Sun: All sources
              echo "scrape_bat=true" >> $GITHUB_OUTPUT
              echo "scrape_carsandbids=true" >> $GITHUB_OUTPUT
              echo "scrape_classic=true" >> $GITHUB_OUTPUT
            fi
          fi

  scrape-model:
    name: Scrape Model
    needs: determine-config
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Display scraping plan
        run: |
          echo "🎯 Scraping Plan:"
          echo "Model: ${{ needs.determine-config.outputs.model }}"
          echo "BaT: ${{ needs.determine-config.outputs.scrape_bat }}"
          echo "Cars & Bids: ${{ needs.determine-config.outputs.scrape_carsandbids }}"
          echo "Classic.com: ${{ needs.determine-config.outputs.scrape_classic }}"
          echo "Max pages: ${{ github.event.inputs.max_pages || '1' }}"

      - name: Run scraper for BaT
        if: needs.determine-config.outputs.scrape_bat == 'true'
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPINGBEE_API_KEY: ${{ secrets.SCRAPINGBEE_API_KEY }}
        run: |
          echo "🚗 Source: Bring a Trailer"

          npx tsx scripts/scraping/scrape-and-save.ts \
            --source=bat-sb \
            --model=${{ needs.determine-config.outputs.model }} \
            --max-pages=${{ github.event.inputs.max_pages || '1' }}

      - name: Run scraper for Cars and Bids
        if: needs.determine-config.outputs.scrape_carsandbids == 'true'
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPINGBEE_API_KEY: ${{ secrets.SCRAPINGBEE_API_KEY }}
        run: |
          echo "🚗 Source: Cars and Bids"

          npx tsx scripts/scraping/scrape-and-save.ts \
            --source=carsandbids-sb \
            --model=${{ needs.determine-config.outputs.model }} \
            --max-pages=${{ github.event.inputs.max_pages || '1' }}

      - name: Run scraper for Classic.com
        if: needs.determine-config.outputs.scrape_classic == 'true'
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          SCRAPINGBEE_API_KEY: ${{ secrets.SCRAPINGBEE_API_KEY }}
        run: |
          echo "🚗 Source: Classic.com"

          npx tsx scripts/scraping/scrape-and-save.ts \
            --source=classic-sb \
            --model=${{ needs.determine-config.outputs.model }} \
            --max-pages=${{ github.event.inputs.max_pages || '1' }}

      - name: Generate report
        if: always()
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          NEXT_PUBLIC_SUPABASE_ANON_KEY: ${{ secrets.NEXT_PUBLIC_SUPABASE_ANON_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          npx tsx -e "
          import { createClient } from '@supabase/supabase-js';

          const supabase = createClient(
            process.env.NEXT_PUBLIC_SUPABASE_URL,
            process.env.SUPABASE_SERVICE_ROLE_KEY
          );

          async function report() {
            const today = new Date().toISOString().split('T')[0];
            const sources = ['bring-a-trailer', 'carsandbids', 'classic'];
            const results = {};

            for (const source of sources) {
              const { count } = await supabase
                .from('listings')
                .select('*', { count: 'exact', head: true })
                .eq('source', source)
                .gte('scraped_at', today + 'T00:00:00.000Z');
              results[source] = count || 0;
            }

            const total = Object.values(results).reduce((a, b) => a + b, 0);

            console.log('📊 Scraping Report');
            console.log('==================');
            console.log(\`Model: ${{ needs.determine-config.outputs.model }}\`);
            console.log(\`Date: \${today}\`);
            console.log('');
            console.log('New listings by source:');
            console.log(\`  BaT: \${results['bring-a-trailer']} (scraped: ${{ needs.determine-config.outputs.scrape_bat }})\`);
            console.log(\`  Cars & Bids: \${results['carsandbids']} (scraped: ${{ needs.determine-config.outputs.scrape_carsandbids }})\`);
            console.log(\`  Classic.com: \${results['classic']} (scraped: ${{ needs.determine-config.outputs.scrape_classic }})\`);
            console.log('');
            console.log(\`Total new listings: \${total}\`);
          }

          report().catch(console.error);
          "

      - name: Report status
        if: always()
        run: |
          DOW=$(date -u +%A)
          if [ "${{ job.status }}" == "success" ]; then
            echo "✅ Daily scraping completed successfully ($DOW)"
          elif [ "${{ job.status }}" == "cancelled" ]; then
            echo "⚠️ Daily scraping was cancelled ($DOW)"
          else
            echo "❌ Daily scraping failed ($DOW)"
            exit 1
          fi