# Scraping Progress Notes - 2025-09-07

**Note: Continued on 2025-09-09 - See `/notes/20250909_scraping.md` for latest updates**

## Current Status (UPDATED - As of 2025-09-08)
- **BaT (Bring a Trailer)**: ✅ WORKING - 226 listings saved
  - Extracting: title, price, year, model, trim, generation
  - Some missing: VIN, mileage
  - Issue: Only getting ~16-24 per run, should get 260+
- **Classic.com**: ✅ FIXED WITH CURL - Now working without Bright Data!
  - Using Python-style curl approach with rate limiting
  - Successfully fetching both search and detail pages
  - Extracting prices, detecting auctions vs sold
- **Edmunds**: ✅ FIXED WITH CURL - Now working without Bright Data!
  - Using Python-style curl approach with rate limiting
  - Successfully fetching pages with correct URL format
- **Cars.com**: ❌ BLOCKED - robots.txt blocking through Bright Data
  - Had 6 listings from earlier test, now blocked
  - Could potentially fix with curl approach
- **Cars and Bids**: ❌ NOT TESTED

**Solution Implemented**: Using curl-based fetching (like Python project) with random delays (2-4.5 seconds between requests) for Classic.com and Edmunds to bypass Bright Data restrictions.

## What We Tried Today

### 1. Fixed BaT Scraper
**Problem**: Parameter bug and no detail pages being fetched
**Solution**: 
- Fixed `options?.maxPages` → `params?.maxPages` bug in bat.ts:91
- Used separate Sets for tracking: `processedUrls` for JSON duplicates, `fetchedDetailUrls` for detail pages
- Successfully extracting: VIN, mileage, colors, transmission, options
**Result**: ✅ Working - fetched 16 sold listings with full data

### 2. Cars.com Integration
**Problem**: No listings being scraped, then "Contact seller" for all titles
**What We Learned**:
- Cars.com embeds data in `window.__CARS_DATA__` or `window.__INITIAL_STATE__` JavaScript variables
- Copied approach from ../porsche project's Python scraper  
- Updated CarsScraper to look for embedded JSON first, then fall back to HTML
**Solution**:
- Created custom `scrapeDetail` method in Cars.com scraper
- Correct selectors: `h1` for title, `span.primary-price` for price
- Mileage in `dt:contains("Mileage") + dd` pattern
- VIN extracted from body text using regex
**Result**: ✅ Successfully extracting all data from Cars.com listings

### 3. Classic.com Issues
**Problem**: Search pages work but detail pages return 502 errors
**What We Learned**:
- Classic.com search pages load fine with listings
- Detail pages are blocked by Bright Data with 502 errors
- The extraction logic is correct but can't be tested due to blocking
**Status**: ❌ Blocked at detail page level

### 4. Database Migrations
**Problem**: Missing columns causing errors
**Solution**: Created migration 006_add_color_columns.sql
**Note**: User ran migration manually - Supabase CLI linking not working

### 4. Gemini API Issues
**Problem**: Quota exceeded errors despite Tier 2 paid plan
**What We Learned**:
- Google AI Studio API has different quotas than Google Cloud API
- Using `gemini-1.5-flash` model instead of `gemini-1.5-pro`
- Added fallback parsing when quota exceeded
**Status**: Still hitting quota limits frequently

## Key Technical Discoveries

### BaT Data Structure
- Embeds auction data in `auctionsCompletedInitialData` JavaScript variable
- VIN found in "Chassis:" field in BaT Essentials list
- Mileage patterns: "8k Miles", "12,345 Miles", etc.
- Colors in description: "finished in [color] over [interior]"

### Bright Data Configuration
- Using zone: `pt_unlocker_z1` for most scraping
- Scraping Browser zone: `pt_scraping_browser_z1` for JavaScript-heavy sites
- CA certificate at: `BrightData SSL certificate (port 33335).crt`
- Some sites (Cars.com) occasionally blocked by robots.txt

### HTML Storage Organization
- Bucket: `raw-html` (NOT `scraped-html`)
- Path structure: `source/yyyymmdd/model/trim/type/filename.html`
- Types: `search` or `detail`
- Example: `bat/20250907/911/gt3/detail/listing_123.html`

## Script Improvements Made
- Added `--source` parameter to scrape-all.ts
- Usage: `npx tsx scripts/scrape-all.ts --source=bat`
- Available sources: bat, classic, carsandbids, edmunds, cars

## Next Steps / TODO
1. Fix Cars.com HTML selectors to properly extract title, price, model
2. Check why Classic.com data extraction is failing (HTML is fetching fine)
3. Test Cars and Bids scraper
4. Consider simpler curl-based approach like Python project uses
5. Add AutoTrader and CarFax as additional sources
6. Set up proper Google Cloud API key instead of AI Studio key
7. Consider implementing Bright Data's Scraping Browser for JS-heavy sites

## Alternative Approach from Python Project
The Python `porsche` project uses a simpler approach:
- Direct `curl` subprocess with proper headers
- No proxy for initial attempts
- Rate limiting with random delays (2-3 seconds base + 0.5-2.5 random)
- Headers: User-Agent, Accept, Accept-Language, Accept-Encoding, Connection, Upgrade-Insecure-Requests

## Environment Variables Used
- `BAT_USERNAME=blim`
- `BAT_PASSWORD=pavfyv-mumzej-miQno3`
- `GEMINI_API_KEY=AIzaSyD7VpPCP44VaTfJB86B5Zp-aMe-b9ddT3E` (AI Studio key, hitting quotas)
- `BRIGHT_DATA_ZONE=pt_unlocker_z1`

## Files Modified Today
- `/lib/scrapers/bat.ts` - Fixed parameter bug, improved data extraction
- `/lib/scrapers/cars.ts` - Added JSON extraction, custom scrapeDetail with proper selectors
- `/lib/scrapers/classic.ts` - Added custom scrapeDetail method + curl fetching
- `/lib/scrapers/edmunds.ts` - Updated URLs + added curl fetching
- `/lib/scrapers/curl-fetcher.ts` - NEW - Python-style curl fetcher with rate limiting
- `/lib/scrapers/shared-scraper.ts` - Added extractYear() and extractMileage() methods
- `/scripts/scrape-all.ts` - Added --source parameter for individual scraper runs
- `/supabase/migrations/006_add_color_columns.sql` - Added exterior_color column
- `.env.local` - Added BaT credentials
- `/notes/20250907_scraping.md` - Created to track progress

## Lessons Learned
1. **Storage is cheap, scraping is not** - Always store raw HTML immediately
2. Many modern sites use JavaScript frameworks that require browser automation
3. Embedded JSON in script tags is often easier to parse than HTML
4. Different Google APIs have different quota systems
5. Always use separate tracking Sets for different purposes (duplicates vs fetched)