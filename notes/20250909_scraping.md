# Scraping Progress Notes - 2025-09-09

## Session Summary
Continued from yesterday's work to fix scrapers that were blocked by Bright Data's robots.txt compliance. Successfully implemented Python-style curl fetching as an alternative to Bright Data proxy.

## Current Working Status

### ✅ Working Scrapers
1. **BaT (Bring a Trailer)**: FIXED! ✅
   - **Solution**: Use Puppeteer with Bright Data Scraping Browser
   - Clicks "Show More" button repeatedly (20+ times)
   - Successfully fetches 390+ listings from single model page
   - The embedded JSON only had 24 items, but HTML has 137+ after loading
   - Must use browser automation to click "Show More" for full results

2. **Classic.com**: FIXED TODAY ✅
   - Switched from Bright Data to curl-based fetching
   - Successfully fetching both search and detail pages
   - Rate limiting: 2s base + 0.5-2.5s random delay
   - Extracting: title, price, VIN, mileage, year
   - Properly detecting auction vs sold status

3. **Edmunds**: FIXED TODAY ✅
   - Switched from Bright Data to curl-based fetching
   - Updated URL format with correct parameters:
     - `inventorytype=used%2Ccpo%2Cnew`
     - `radius=6000` (nationwide)
     - `year=2022-*` (for GT4 RS example)
   - Successfully fetching pages with rate limiting

### ❌ Not Working
1. **Cars.com**: BLOCKED
   - robots.txt blocking through Bright Data
   - Had 6 listings from earlier test
   - Could potentially fix with curl approach (not implemented yet)

2. **Cars and Bids**: NOT TESTED
   - Haven't attempted yet
   - Likely also blocked by Bright Data

## Key Changes Made Today

### 1. Created Curl-Based Fetcher
**File**: `/lib/scrapers/curl-fetcher.ts`
- Mimics Python project's approach
- Rate limiting per domain (2-3 seconds base)
- Random delay (0.5-2.5 seconds additional)
- Retry logic with exponential backoff
- Headers matching Python project:
  ```
  User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)
  Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
  Accept-Language: en-US,en;q=0.5
  Accept-Encoding: gzip, deflate
  Connection: keep-alive
  Upgrade-Insecure-Requests: 1
  ```

### 2. Updated Classic.com Scraper
**File**: `/lib/scrapers/classic.ts`
- Added `fetchUrl()` override to use curl instead of Bright Data
- Still stores HTML in Supabase storage
- Maintains session cache
- Working selectors:
  - Title: `h1`
  - Price: `.text-xl.font-medium`
  - VIN/Mileage: Extracted via regex from body text

### 3. Updated Edmunds Scraper
**File**: `/lib/scrapers/edmunds.ts`
- Added `fetchUrl()` override to use curl
- Fixed URL format based on user's example
- Parameters: `inventorytype`, `make`, `model`, `radius`, `year`, `keywords`

## Issues Discovered

### Bright Data Limitations
- Error: "Requested site is not available for immediate access mode in accordance with robots.txt"
- Affects: Cars.com, Edmunds (before curl fix), Classic.com detail pages (before fix)
- Solution: Bypass Bright Data entirely using direct curl requests

### BaT Scraper FIXED with Puppeteer!
- Issue: Embedded JSON only contains 24 items (recently sold)
- Solution: Use Puppeteer to click "Show More" button repeatedly
- Results: Now getting 390+ listings from single model page!
- Key insight: BaT uses dynamic loading, not static pagination
- The `/page/2/` URLs don't work - must use JavaScript clicking

### Database Quality Issues
- Some BaT listings are parts (seats, wheels) not cars
- Missing VIN/mileage for many BaT listings
- No options/colors being extracted for most scrapers

## Technical Learnings

1. **Curl vs Bright Data**: Direct curl requests with proper headers and rate limiting work better than proxy services for many sites

2. **Rate Limiting is Critical**: Random delays (2-4.5 seconds total) prevent blocking

3. **HTML Storage**: Continue storing all HTML in Supabase even when bypassing proxy

4. **Robots.txt Compliance**: Bright Data strictly enforces robots.txt, making it unsuitable for some sites

## Next Steps

1. ✅ **DONE - BaT listing count fixed**: Now getting 390+ listings using Puppeteer
2. **Run full BaT scrape**: Use bat-puppeteer.ts for all model pages
3. **Implement curl for Cars.com**: Apply same approach to bypass robots.txt
4. **Test Cars and Bids**: See if it works with current approach
5. **Improve data extraction**: Add color, transmission, options extraction

## Command Reference
```bash
# Run individual scrapers
npx tsx scripts/scrape-all.ts --source=bat  # OLD - only gets 16-24
npx tsx scripts/scrape-bat-puppeteer.ts      # NEW - gets 390+ per model!
npx tsx scripts/scrape-all.ts --source=classic
npx tsx scripts/scrape-all.ts --source=edmunds
npx tsx scripts/scrape-all.ts --source=cars
npx tsx scripts/scrape-all.ts --source=carsandbids

# Test BaT Puppeteer scraper
npx tsx scripts/test-bat-puppeteer.ts

# Check recent listings in database
npx tsx scripts/check-recent-listings.ts
```

## Files Modified Today
- `/lib/scrapers/curl-fetcher.ts` - NEW - Python-style curl fetcher
- `/lib/scrapers/classic.ts` - Added fetchUrl override for curl
- `/lib/scrapers/edmunds.ts` - Added fetchUrl override for curl, fixed URLs
- `/lib/scrapers/bright-data-puppeteer.ts` - FIXED selectors to extract titles/URLs correctly
- `/scripts/analyze-bat-full.ts` - Updated to analyze BaT pagination structure
- `/notes/20250909_scraping.md` - Created this file

## Environment Variables
- All remain the same from yesterday
- Bright Data credentials still in place but not used for Classic/Edmunds